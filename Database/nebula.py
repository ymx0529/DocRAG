from typing import Dict, List
from nebula3.gclient.net import ConnectionPool
from nebula3.Config import Config
from nebula3.common import ttypes
import time


# é¢„å®šä¹‰ TAG åç§°
PREDEFINED_TAGS = [
    "ENTITY_NODE",
    "SEGMENT ANCHOR NODE",
    "ORIGINAL_IMAGE",
    "EQUATION_NODE",
    "TABLE_NODE",
]

ENTITY_TAG = PREDEFINED_TAGS[0]
SPECIAL_TAGS = set(PREDEFINED_TAGS[1:])

class NebulaHandler:
    """
    Nebula handler:
    - å›ºå®šäº”ä¸ªé¢„å®šä¹‰ TAG
    - ENTITY_NODE é¡¶ç‚¹ vid_type=FIXED_STRING(64)ï¼Œä½¿ç”¨ entityID ä½œä¸º vid
    - ENTITY_NODE é™„åŠ å­—æ®µï¼šname, description, type, chunkID
    - ç»Ÿä¸€è¾¹ç±»åž‹ï¼šRelatesTo(relationship string, relationship_strength double)
    """

    def __init__(self, space_name: str = "mrag_test", host: str = "127.0.0.1", port: int = 9669, user: str = "root", password: str = "nebula"):
        self.space_name = space_name
        config = Config()
        self.pool = ConnectionPool()
        ok = self.pool.init([(host, port)], config)
        if not ok:
            raise RuntimeError("Nebula ConnectionPool init failed")
        self.session = self.pool.get_session(user, password)
        self._ensure_space_exists()

        # name -> entityID æ˜ å°„
        self.name_to_vid: Dict[str, str] = {}

    def _normalize_vid(self, vid: str) -> str:
        """ç¡®ä¿ Nebula vid ç¬¦åˆ FIXED_STRING(64) è¦æ±‚"""
        if not isinstance(vid, str):
            vid = str(vid)

        # ðŸ”¹ å¦‚æžœè¶…è¿‡ 128ï¼Œç›´æŽ¥æˆªæ–­å‰ 128 ä½
        if len(vid) > 128:
            return vid[:128]
        return vid

    """ def _exec(self, nGQL: str):
        res = self.session.execute(nGQL)
        if res.error_code() != ttypes.ErrorCode.SUCCEEDED:
            raise RuntimeError(f"Nebula nGQL failed: {nGQL}\n{res.error_msg()}")
        return res """
    
    def _exec(self, nGQL: str, retries: int = 20, retry_interval: float = 1.0):
        for attempt in range(retries):
            try:
                # å°è¯•æ‰§è¡Œ nGQL æŸ¥è¯¢
                res = self.session.execute(nGQL)
                if res.error_code() == ttypes.ErrorCode.SUCCEEDED:
                    return res
                if "Not the leader" in res.error_msg():
                    print(f"[WARN] Not the leader, ç¬¬ {attempt+1} æ¬¡é‡è¯• ...")
                else:
                    # å¦‚æžœå‡ºçŽ°å…¶ä»–é”™è¯¯ï¼Œæ‰“å°å¹¶é‡è¯•
                    print(f"[WARN] æ‰§è¡Œå¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯: {res.error_msg()}ï¼Œç¬¬ {attempt+1} æ¬¡é‡è¯• ...")
                
            except Exception as e:
                # æ•èŽ·æ‰€æœ‰ä¼ è¾“å¼‚å¸¸æˆ–å…¶ä»–æ‰€æœ‰å¼‚å¸¸ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•
                print(f"[WARN] å¼‚å¸¸: {str(e)}ï¼Œç¬¬ {attempt+1} æ¬¡é‡è¯• ...")
            
            time.sleep(retry_interval)
        raise RuntimeError(f"[ERROR] é‡è¯• {retries} æ¬¡åŽä»ç„¶å¤±è´¥: {nGQL}")

    def _ensure_space_exists(self):
        self._exec(
            f'CREATE SPACE IF NOT EXISTS {self.space_name}(partition_num=1, replica_factor=1, vid_type=FIXED_STRING(128));'
        )
        # ç­‰å¾… space å‡ºçŽ°åœ¨ SHOW SPACES
        for i in range(30):  # æœ€å¤šç­‰ 30 ç§’
            res = self._exec("SHOW SPACES;")
            spaces = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
            if self.space_name in spaces:
                break
            time.sleep(1)
        else:
            raise RuntimeError(f"Space {self.space_name} ä¸€ç›´æœªå‡ºçŽ°åœ¨ SHOW SPACES")
        self._exec(f'USE {self.space_name};')

    def create_schema(self):
        """
        ç¡®ä¿ space/schema å­˜åœ¨ï¼Œä½†ä¸ä¼šåˆ é™¤å·²æœ‰æ•°æ®
        """
        # å¦‚æžœç©ºé—´ä¸å­˜åœ¨åˆ™åˆ›å»º
        self._exec(
            f'CREATE SPACE IF NOT EXISTS {self.space_name}('
            f'partition_num=1, replica_factor=1, vid_type=FIXED_STRING(128));'
        )

        # å°è¯• USE ç©ºé—´
        for i in range(30):
            try:
                self._exec(f'USE {self.space_name};')
                print(f"[Nebula] Space {self.space_name} å·²ç¡®è®¤å¯ç”¨ âœ…")
                break
            except Exception:
                time.sleep(1)
        else:
            raise RuntimeError(f"Space {self.space_name} ä¸€ç›´æ— æ³• USE")

        # åˆ›å»º TAG & EDGEï¼ˆä¸ä¼šè¦†ç›–å·²æœ‰æ•°æ®ï¼‰
        self._exec(
            f'CREATE TAG IF NOT EXISTS `{ENTITY_TAG}`('
            f'name string, description string, type string, chunkID string);'
        )
        for tag in SPECIAL_TAGS:
            self._exec(
                f'CREATE TAG IF NOT EXISTS `{tag}`(name string, description string, chunkID string);'
            )
        self._exec(
            'CREATE EDGE IF NOT EXISTS RelatesTo(relationship string, relationship_strength double);'
        )

        # ç¡®è®¤ TAG å·²å­˜åœ¨
        for i in range(30):
            res = self._exec("SHOW TAGS;")
            tags = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
            if "ENTITY_NODE" in tags:
                print("[Nebula] ENTITY_NODE schema å·²ç¡®è®¤å­˜åœ¨ âœ…")
                break
            time.sleep(1)
        else:
            raise RuntimeError("ENTITY_NODE schema æœªæˆåŠŸåˆ›å»º")



    def reset_space(self):
        print(f"[DEBUG] å°è¯•åˆ é™¤ç©ºé—´ {self.space_name}")
        self._exec(f'DROP SPACE IF EXISTS {self.space_name};')
        time.sleep(2)

        print(f"[DEBUG] é‡æ–°åˆ›å»ºç©ºé—´ {self.space_name}")
        self._exec(
            f'CREATE SPACE IF NOT EXISTS {self.space_name}('
            f'partition_num=1, replica_factor=1, vid_type=FIXED_STRING(128));'
        )

        # ç­‰å¾… space å‡ºçŽ°åœ¨ SHOW SPACES
        for i in range(30):
            res = self._exec("SHOW SPACES;")
            spaces = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
            if self.space_name in spaces:
                break
            time.sleep(1)
        else:
            raise RuntimeError(f"Space {self.space_name} ä¸€ç›´æœªå‡ºçŽ°åœ¨ SHOW SPACES")

        # å†ç­‰å¾… USE æˆåŠŸ
        for i in range(30):
            try:
                self._exec(f'USE {self.space_name};')
                print(f"[Nebula] Space {self.space_name} å·²ç¡®è®¤å¯ç”¨ âœ…")
                break
            except Exception:
                time.sleep(1)
        else:
            raise RuntimeError(f"Space {self.space_name} ä¸€ç›´æ— æ³• USE")

        # ðŸ”¹ åˆ›å»º TAG & EDGE
        self._exec(
            f'CREATE TAG IF NOT EXISTS `{ENTITY_TAG}`('
            f'name string, description string, type string, chunkID string);'
        )
        for tag in SPECIAL_TAGS:
            self._exec(
                f'CREATE TAG IF NOT EXISTS `{tag}`(name string, description string, chunkID string);'
            )
        self._exec(
            'CREATE EDGE IF NOT EXISTS RelatesTo(relationship string, relationship_strength double);'
        )

        # ðŸ”¹ ç¡®è®¤ TAG ç”Ÿæ•ˆ
        for i in range(30):
            res = self._exec("SHOW TAGS;")
            tags = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
            print("[DEBUG] å½“å‰ TAGS:", tags)
            if "ENTITY_NODE" in tags:
                print("[Nebula] ENTITY_NODE schema å·²ç¡®è®¤åˆ›å»ºæˆåŠŸ âœ…")
                break
            time.sleep(1)
        else:
            raise RuntimeError("ENTITY_NODE schema æœªæˆåŠŸåˆ›å»º")

        # ðŸ”¹ å¼ºåˆ¶è§¦å‘æ‰€æœ‰ TAG çš„ schema åŒæ­¥
        for tag in [ENTITY_TAG] + list(SPECIAL_TAGS):
            for i in range(10):
                try:
                    self._exec(f'DESC TAG `{tag}`;')
                    print(f"[Nebula] DESC TAG {tag} æˆåŠŸï¼ŒStorage å·²åŒæ­¥ âœ…")
                    break
                except Exception as e:
                    print(f"[DEBUG] DESC TAG {tag} ç¬¬ {i+1} æ¬¡å¤±è´¥ï¼Œç­‰å¾…åŒæ­¥: {e}")
                    time.sleep(1)
            else:
                raise RuntimeError(f"{tag} schema ä¸€ç›´æœªåŒæ­¥åˆ° Storage")

        # ðŸ”¹ æ–°å¢ž: å¼ºåˆ¶è§¦å‘ EDGE çš„ schema åŒæ­¥
        for i in range(10):
            try:
                self._exec('DESC EDGE RelatesTo;')
                print("[Nebula] DESC EDGE RelatesTo æˆåŠŸï¼ŒStorage å·²åŒæ­¥ âœ…")
                break
            except Exception as e:
                print(f"[DEBUG] DESC EDGE RelatesTo ç¬¬ {i+1} æ¬¡å¤±è´¥ï¼Œç­‰å¾…åŒæ­¥: {e}")
                time.sleep(1)
        else:
            raise RuntimeError("RelatesTo edge schema ä¸€ç›´æœªåŒæ­¥åˆ° Storage")
        
        #time.sleep(10) 

        # Dummy èŠ‚ç‚¹å†™å…¥æµ‹è¯•ï¼ˆé™é»˜é‡è¯•ï¼Œç›´åˆ°æˆåŠŸï¼‰
        test_vid = self._normalize_vid("DummyTestNode001")   # å›ºå®šæµ‹è¯•ç‚¹ ID
        last_err = None

        for i in range(60):  # æœ€å¤šé‡è¯• 60 æ¬¡ï¼Œç­‰å¾…1åˆ†é’Ÿ
            try:
                nGQL = (
                    f'INSERT VERTEX `{ENTITY_TAG}`(name, description, type, chunkID) '
                    f'VALUES "{test_vid}":("tmp_name", "tmp_desc", "tmp_type", "0");'
                )
                self._exec(nGQL)
                self._exec(f'FETCH PROP ON `{ENTITY_TAG}` "{test_vid}" YIELD properties(vertex);')
                self._exec(f'DELETE VERTEX "{test_vid}";')  # åˆ é™¤æµ‹è¯•ç‚¹
                print(f"[Nebula] Dummy èŠ‚ç‚¹å†™å…¥æˆåŠŸ âœ… (ç¬¬ {i+1} æ¬¡å°è¯•)")
                break
            except Exception as e:
                msg = str(e)
                last_err = msg
                time.sleep(1)
                continue
        else:
            print(f"[WARN] Dummy èŠ‚ç‚¹å†™å…¥åœ¨ 20 æ¬¡å°è¯•åŽä»æœªæˆåŠŸï¼Œæœ€åŽé”™è¯¯: {last_err}")



    # ---------- å®žä½“å†™å…¥ ----------
    def insert_entity(self, entityID: str, type_name: str, name: str, description: str = "", chunkID: str = ""):
        self._exec(f'USE {self.space_name};')
        
        # ðŸ”¹ ä¿®æ”¹ç‚¹ : æ¯æ¬¡æ’å…¥å‰å¼ºåˆ¶ USE
        self._exec(f'USE {self.space_name};')

        # Step1: ç¡®è®¤ TAG æ˜¯å¦å­˜åœ¨
        res = self._exec("SHOW TAGS;")
        tags = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
        if "ENTITY_NODE" not in tags:
            raise RuntimeError("[ERROR] ENTITY_NODE schema æœªæ‰¾åˆ°ï¼Œè¯·ç¡®è®¤ reset_space æ˜¯å¦æˆåŠŸæ‰§è¡Œ")

        target_tag = type_name if type_name in SPECIAL_TAGS else ENTITY_TAG

        # æ¸…æ´—
        safe_desc = description.replace('\\n', ' ').replace('\\r', ' ').replace('\\', '\\\\').replace('"', '\\"')
        safe_name = name.replace('\\n', ' ').replace('\\r', ' ').replace('\\', '\\\\').replace('"', '\\"')
        safe_chunk = chunkID.replace('\\n', ' ').replace('\\r', ' ').replace('\\', '\\\\').replace('"', '\\"')

        if target_tag == ENTITY_TAG:
            nGQL = (
                f'INSERT VERTEX `{ENTITY_TAG}`(name, description, type, chunkID) '
                f'VALUES "{entityID}":("{safe_name}", "{safe_desc}", "{type_name}", "{safe_chunk}");'
            )

        elif target_tag == PREDEFINED_TAGS[1]:
            nGQL = (
                f'INSERT VERTEX `{target_tag}`(name, description) '
                f'VALUES "{entityID}":("{safe_name}", "{safe_desc}");'
            )

        else:
            nGQL = (
                f'INSERT VERTEX `{target_tag}`(name, description, chunkID) '
                f'VALUES "{entityID}":("{safe_name}", "{safe_desc}", "{safe_chunk}");'
            )

        self._exec(nGQL)
        time.sleep(0.05)
        verify = self._exec(f'FETCH PROP ON `{target_tag}` "{entityID}" YIELD properties(vertex);')
        print(f"[Nebula] Inserted `{target_tag}` {entityID}: ", [r.values for r in verify.rows()])

        # å»ºç«‹æ˜ å°„ï¼šname -> entityID
        self.name_to_vid[name] = entityID

    # ---------- è¾¹å†™å…¥ ----------
    def insert_relation(self, src: str, dst: str,
                        relationship: str, relationship_strength: float):
        self._exec(f'USE {self.space_name};')

        def sanitize(s: str) -> str:
            return s.replace('\\n', ' ').replace('\\r', ' ').replace('\\', '\\\\').replace('"', '\\"')

        src, dst, relationship = sanitize(src), sanitize(dst), sanitize(relationship)

        # å¦‚æžœä¼ å…¥çš„æ˜¯ nameï¼Œç”¨æ˜ å°„è½¬æˆ vid
        src_vid = self.name_to_vid.get(src, src)
        dst_vid = self.name_to_vid.get(dst, dst)

        print(f"[Nebula] Inserting edge: raw {src} -> {dst} => resolved {src_vid} -> {dst_vid}")

        nGQL = (
            f'INSERT EDGE RelatesTo(relationship, relationship_strength) '
            f'VALUES "{src_vid}"->"{dst_vid}":("{relationship}", {float(relationship_strength)});'
        )
        self._exec(nGQL)
        time.sleep(0.05)
        verify = self._exec(
            f'FETCH PROP ON RelatesTo "{src_vid}"->"{dst_vid}" YIELD properties(edge);'
        )
        print(f"[Nebula] Inserted Relation {src_vid}->{dst_vid}: ", [r.values for r in verify.rows()])

    
    # -------------------  æ‰¹é‡å†™å…¥ï¼ˆæ–°å¢žéƒ¨åˆ†ï¼‰ -------------------
    def insert_entities_bulk(self, entities: List[Dict], batch_size: int = 1000):
        """æ‰¹é‡æ’å…¥å®žä½“"""
        self._exec(f'USE {self.space_name};')

        #  ç¡®è®¤ TAG æ˜¯å¦å­˜åœ¨
        res = self._exec("SHOW TAGS;")
        tags = [row.values[0].get_sVal().decode("utf-8") for row in res.rows()]
        print("[DEBUG] insert_entities_bulk å½“å‰ TAGS:", tags)
        if "ENTITY_NODE" not in tags:
            raise RuntimeError("[ERROR] ENTITY_NODE schema æœªæ‰¾åˆ°ï¼Œæ— æ³•æ‰¹é‡æ’å…¥")

         #  äºŒæ¬¡æ ¡éªŒ TAG æ˜¯å¦å­˜åœ¨
        tags = self._exec("SHOW TAGS;")
        tags_list = [row.values[0].get_sVal().decode("utf-8") for row in tags.rows()]
        if ENTITY_TAG not in tags_list:
            raise RuntimeError("ENTITY_NODE TAG ä¸å­˜åœ¨ï¼Œè¯·ç¡®è®¤ reset_space æ˜¯å¦æˆåŠŸæ‰§è¡Œ")
        
        buffer = {}  # ä¸åŒ TAG åˆ†å¼€ç¼“å­˜

        for e in entities:
            raw_id = e["entityID"]
            entityID = self._normalize_vid(raw_id)   # âœ… ä½¿ç”¨è§„èŒƒåŒ– vid

            original_name = e.get("name", "")
            self.name_to_vid[original_name] = entityID

            type_name = e.get("type", "")
            name = e.get("name", "").replace('"', '\\"')
            desc = e.get("description", "").replace('"', '\\"')
            chunkID = e.get("chunkID", "").replace('"', '\\"')

            target_tag = type_name if type_name in SPECIAL_TAGS else ENTITY_TAG

            if target_tag == ENTITY_TAG:
                values = f'"{entityID}":("{name}", "{desc}", "{type_name}", "{chunkID}")'
            elif target_tag in SPECIAL_TAGS:
                # special TAG æ²¡æœ‰ type å­—æ®µ
                values = f'"{entityID}":("{name}", "{desc}", "{chunkID}")'
            else:
                continue

            if target_tag not in buffer:
                buffer[target_tag] = []
            buffer[target_tag].append(values)

            # åˆ†æ‰¹æ’å…¥
            if len(buffer[target_tag]) >= batch_size:
                self._flush_entities(buffer[target_tag], target_tag)
                buffer[target_tag].clear()

        # æ’å…¥å‰©ä½™çš„
        for tag, values in buffer.items():
            if values:
                self._flush_entities(values, tag)

    
    def _flush_entities(self, values: List[str], tag: str, retries: int = 5, retry_interval: float = 1.0):
        """æ‰¹é‡æ’å…¥å®žä½“"""
        if tag == ENTITY_TAG:
            nGQL = f'INSERT VERTEX `{ENTITY_TAG}`(name, description, type, chunkID) VALUES {",".join(values)};'
        else:
            nGQL = f'INSERT VERTEX `{tag}`(name, description, chunkID) VALUES {",".join(values)};'

        last_err = None
        for _ in range(retries):
            try:
                self._exec(nGQL)
                print(f"[Nebula] Bulk inserted {len(values)} entities into {tag}")
                return  #  æˆåŠŸç«‹å³è¿”å›ž
            except Exception as e:
                msg = str(e)
                last_err = msg
                if "Not the leader" in msg or "Storage Error" in msg:
                    time.sleep(retry_interval)
                    continue
                raise  # å…¶ä»–é”™è¯¯ç›´æŽ¥æŠ›å‡º
        raise RuntimeError(f"[ERROR] æ’å…¥ {tag} æ‰¹é‡æ•°æ®å¤±è´¥è¶…è¿‡ {retries} æ¬¡ï¼Œæœ€åŽé”™è¯¯: {last_err}")

    def insert_relations_bulk(self, relations: List[Dict], batch_size: int = 1000):
        """æ‰¹é‡æ’å…¥è¾¹"""
        
        values = []
        for r in relations:
            raw_src = r["source"]
            raw_dst = r["target"]

            src_vid = self.name_to_vid.get(raw_src, raw_src)
            dst_vid = self.name_to_vid.get(raw_dst, raw_dst)
            src = self._normalize_vid(src_vid)
            dst = self._normalize_vid(dst_vid)

            rel = r.get("relationship", "").replace('"', '\\"')
            strength = float(r.get("relationship_strength", 0.0))

            values.append(f'"{src}"->"{dst}":("{rel}", {strength})')
           
            if len(values) >= batch_size:
                self._flush_relations(values)
                values.clear()

        if values:
            self._flush_relations(values)
        

    def _flush_relations(self, values: List[str], retries: int = 5, retry_interval: float = 1.0):
        """æ‰¹é‡æ’å…¥å…³ç³»"""
        self._exec(f'USE {self.space_name};')
        nGQL = f'INSERT EDGE RelatesTo(relationship, relationship_strength) VALUES {",".join(values)};'
        

        last_err = None
        for _ in range(retries):
            try:
                self._exec(nGQL)
                print(f"[Nebula] Bulk inserted {len(values)} relations")  #  æˆåŠŸæ‰“å°
                return
            except Exception as e:
                msg = str(e)
                last_err = msg
                time.sleep(retry_interval)
                continue
        
        raise RuntimeError(f"[ERROR] æ’å…¥å…³ç³»æ‰¹é‡æ•°æ®å¤±è´¥è¶…è¿‡ {retries} æ¬¡ï¼Œæœ€åŽé”™è¯¯: {last_err}")


        # ---------- æ£€ç´¢é‚»å±… ----------
    def fetch_neighbors_2_hops(self, entityID: str) -> Dict[str, List[str]]:
        self._exec(f'USE {self.space_name};')

        q1 = f'GO 1 STEPS FROM "{entityID}" OVER RelatesTo YIELD DISTINCT dst(edge) as dst;'
        r1 = self._exec(q1)
        hop1 = {row.values[0].get_sVal().decode("utf-8") for row in r1.rows()}
        hop1.add(entityID)  # åŠ å…¥è‡ªèº«
        hop1 = sorted(hop1)

        q2 = f'GO 2 STEPS FROM "{entityID}" OVER RelatesTo YIELD DISTINCT dst(edge) as dst;'
        r2 = self._exec(q2)
        hop2_raw = {row.values[0].get_sVal().decode("utf-8") for row in r2.rows()}
        hop2 = sorted([v for v in hop2_raw if v != entityID and v not in hop1])

        print(f"[Nebula] Neighbors for {entityID}: 1-hop={hop1}, 2-hop={hop2}")
        return {"1-hop": hop1, "2-hop": hop2}

    def fetch_neighbors_1_hop(self, entityID: str) -> Dict[str, List[str]]:
        self._exec(f'USE {self.space_name};')

        q1 = f'GO 1 STEPS FROM "{entityID}" OVER RelatesTo YIELD DISTINCT dst(edge) as dst;'
        r1 = self._exec(q1)
        hop1 = {row.values[0].get_sVal().decode("utf-8") for row in r1.rows()}
        hop1.add(entityID)  # åŠ å…¥è‡ªèº«
        hop1 = sorted(hop1)

        print(f"[Nebula] Neighbors for {entityID}: 1-hop={hop1}")
        return {"1-hop": hop1}


    def close(self):
        try:
            self.session.release()
        finally:
            self.pool.close()
    
    def compute_and_export_algorithms(
        self,
        pagerank_csv: str = "pagerank.csv",
        closeness_csv: str = "closeness.csv",
        pagerank_alpha: float = 0.85,
        pagerank_tol: float = 1e-6,
        pagerank_max_iter: int = 100,
        use_edge_weight: bool = False,
        closeness_undirected: bool = True,
        warn_node_threshold: int = 200000,
    ):
        """
        ä½¿ç”¨ MATCH æŠ“å–æ‰€æœ‰èŠ‚ç‚¹å’Œè¾¹ï¼Œè®¡ç®— PageRank å’Œ Closenessï¼Œ
        è¾“å‡ºä¸¤ä¸ª CSV æ–‡ä»¶ï¼ˆvid,valueï¼‰ã€‚
        """
        import networkx as nx
        import pandas as pd

        self._exec(f'USE {self.space_name};')

        # 1) èŽ·å–æ‰€æœ‰èŠ‚ç‚¹ id
        nodes = set()
        tags_res = self._exec("SHOW TAGS;")
        tags = []
        for row in tags_res.rows():
            try:
                tags.append(row.values[0].get_sVal().decode("utf-8"))
            except Exception:
                tags.append(str(row.values[0]))

        for tag in tags:
            q = f'MATCH (v:`{tag}`) RETURN id(v);'
            r = self._exec(q)
            for row in r.rows():
                try:
                    vid = row.values[0].get_sVal().decode("utf-8")
                except Exception:
                    vid = str(row.values[0])
                nodes.add(vid)

        # 2) èŽ·å–æ‰€æœ‰ RelatesTo è¾¹
        edges = []
        if use_edge_weight:
            q = (
                "MATCH (a)-[e:RelatesTo]->(b) "
                "RETURN id(a) AS src, id(b) AS dst, e.relationship_strength AS weight;"
            )
        else:
            q = "MATCH (a)-[e:RelatesTo]->(b) RETURN id(a) AS src, id(b) AS dst;"
        r = self._exec(q)
        for row in r.rows():
            src = row.values[0].get_sVal().decode("utf-8")
            dst = row.values[1].get_sVal().decode("utf-8")
            w = 1.0
            if use_edge_weight and len(row.values) > 2:
                try:
                    wval = row.values[2]
                    if hasattr(wval, "get_dVal"):
                        w = float(wval.get_dVal())
                    elif hasattr(wval, "get_fVal"):
                        w = float(wval.get_fVal())
                    elif hasattr(wval, "get_iVal"):
                        w = float(wval.get_iVal())
                    else:
                        sval = wval.get_sVal()
                        if sval is not None:
                            w = float(sval.decode("utf-8"))
                except Exception:
                    w = 1.0
            edges.append((src, dst, float(w)))

        # 3) æž„å»ºå›¾
        G = nx.DiGraph()
        G.add_nodes_from(nodes)
        for s, d, w in edges:
            if s not in G:
                G.add_node(s)
            if d not in G:
                G.add_node(d)
            G.add_edge(s, d, weight=w)

        n_nodes, n_edges = G.number_of_nodes(), G.number_of_edges()
        print(f"[Nebula] Graph built: nodes={n_nodes}, edges={n_edges}")

        if n_nodes == 0:
            raise RuntimeError("Graph is empty (no nodes). Aborting algorithm computation.")
        if n_nodes > warn_node_threshold:
            print(f"[Warning] èŠ‚ç‚¹æ•° {n_nodes} è¶…è¿‡é˜ˆå€¼ {warn_node_threshold}ï¼Œnetworkx è®¡ç®—å¯èƒ½å¾ˆæ…¢æˆ–å†…å­˜ä¸è¶³ã€‚")

        # 4) PageRank
        try:
            pr = nx.pagerank(
                G,
                alpha=pagerank_alpha,
                tol=pagerank_tol,
                max_iter=pagerank_max_iter,
                weight="weight" if use_edge_weight else None,
            )
        except Exception as e:
            print(f"[Nebula] PageRankï¼ˆå¸¦æƒï¼‰å¤±è´¥ï¼Œé€€å›žæ— æƒè®¡ç®—ï¼š{e}")
            pr = nx.pagerank(G, alpha=pagerank_alpha, tol=pagerank_tol, max_iter=pagerank_max_iter)

        # 5) Closeness
        if closeness_undirected:
            H = G.to_undirected()
            clos = nx.closeness_centrality(H)
        else:
            clos = nx.closeness_centrality(G)

        # 6) è¾“å‡º CSV
        pr_df = pd.DataFrame(list(pr.items()), columns=["_id", "_pagerank"]).sort_values("_pagerank", ascending=False)
        pr_df.to_csv(pagerank_csv, index=False, float_format="%.12g")

        clos_df = pd.DataFrame(list(clos.items()), columns=["_id", "_closeness"]).sort_values("_closeness", ascending=False)
        clos_df.to_csv(closeness_csv, index=False, float_format="%.12g")

        print(f"[Nebula] Saved PageRank -> {pagerank_csv} (rows={len(pr_df)})")
        print(f"[Nebula] Saved Closeness -> {closeness_csv} (rows={len(clos_df)})")

        return pr, clos


        # æŸ¥è¯¢èŠ‚ç‚¹æ•°é‡
    def get_node_count(self):
        # ç»Ÿè®¡æ¯ä¸ªæ ‡ç­¾çš„èŠ‚ç‚¹æ•°é‡
        query_entity_node = "MATCH (v:ENTITY_NODE) RETURN COUNT(v) AS entity_node_count"
        res_entity_node = self._exec(query_entity_node)
        entity_node_count = res_entity_node.rows()[0].values[0].get_iVal()

        query_segment_anchor_node = "MATCH (v:`SEGMENT ANCHOR NODE`) RETURN COUNT(v) AS segment_anchor_node_count"
        res_segment_anchor_node = self._exec(query_segment_anchor_node)
        segment_anchor_node_count = res_segment_anchor_node.rows()[0].values[0].get_iVal()

        query_original_image = "MATCH (v:`ORIGINAL_IMAGE`) RETURN COUNT(v) AS original_image_node_count"
        res_original_image = self._exec(query_original_image)
        original_image_node_count = res_original_image.rows()[0].values[0].get_iVal()

        return {
            "ENTITY_NODE": entity_node_count,
            "SEGMENT ANCHOR NODE": segment_anchor_node_count,
            "ORIGINAL_IMAGE": original_image_node_count
        }

    # æŸ¥è¯¢è¾¹æ•°é‡
    def get_edge_count(self):
        query = "MATCH ()-[e:RelatesTo]->() RETURN COUNT(e)"
        res = self._exec(query)
        return res.rows()[0].values[0].get_iVal()
